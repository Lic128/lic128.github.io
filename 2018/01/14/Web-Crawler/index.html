<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="HTTP基本原理URL： Universal Resource Locator， 统一资源定位符。 URI： Uniform Resource Identifier， 统一资源标识符。 URL 是 URI 的子集，也就是说每个 URL 都是 URI，但不是每个 URI 都是 URL。那么怎样的 URI 不是 URL 呢">
<meta property="og:type" content="article">
<meta property="og:title" content="Web Crawler">
<meta property="og:url" content="http://yoursite.com/2018/01/14/Web-Crawler/index.html">
<meta property="og:site_name" content="Rock of SisyPhus">
<meta property="og:description" content="HTTP基本原理URL： Universal Resource Locator， 统一资源定位符。 URI： Uniform Resource Identifier， 统一资源标识符。 URL 是 URI 的子集，也就是说每个 URL 都是 URI，但不是每个 URI 都是 URL。那么怎样的 URI 不是 URL 呢？URI 还包括一个子类叫做 URN，它的全称为 Universal Resou">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://germey.gitbooks.io/python3webspider/content/assets/2-3.png">
<meta property="og:image" content="https://germey.gitbooks.io/python3webspider/content/assets/2-8.jpg">
<meta property="og:image" content="https://germey.gitbooks.io/python3webspider/content/assets/2-9.jpg">
<meta property="og:image" content="https://germey.gitbooks.io/python3webspider/content/assets/3-2.jpg">
<meta property="og:image" content="https://germey.gitbooks.io/python3webspider/content/assets/3-9.jpg">
<meta property="og:image" content="https://germey.gitbooks.io/python3webspider/content/assets/3-10.jpg">
<meta property="og:updated_time" content="2018-01-17T23:45:34.358Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Web Crawler">
<meta name="twitter:description" content="HTTP基本原理URL： Universal Resource Locator， 统一资源定位符。 URI： Uniform Resource Identifier， 统一资源标识符。 URL 是 URI 的子集，也就是说每个 URL 都是 URI，但不是每个 URI 都是 URL。那么怎样的 URI 不是 URL 呢？URI 还包括一个子类叫做 URN，它的全称为 Universal Resou">
<meta name="twitter:image" content="https://germey.gitbooks.io/python3webspider/content/assets/2-3.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/01/14/Web-Crawler/"/>





  <title>Web Crawler | Rock of SisyPhus</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Rock of SisyPhus</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/14/Web-Crawler/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Liyan Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Rock of SisyPhus">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Web Crawler</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-01-14T16:46:17-08:00">
                2018-01-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="HTTP基本原理"><a href="#HTTP基本原理" class="headerlink" title="HTTP基本原理"></a>HTTP基本原理</h2><p>URL： Universal Resource Locator， 统一资源定位符。</p>
<p>URI： Uniform Resource Identifier， 统一资源标识符。</p>
<p>URL 是 URI 的子集，也就是说每个 URL 都是 URI，但不是每个 URI 都是 URL。那么怎样的 URI 不是 URL 呢？URI 还包括一个子类叫做 URN，它的全称为 Universal Resource Name，即统一资源名称。URN 只命名资源而不指定如何定位资源，如 urn:isbn:0451450523，它指定了一本书的 ISBN，可以唯一标识这一本书，但是没有指定到哪里定位这本书，这就是 URN。</p>
<p>HTTP 的全称是 Hyper Text Transfer Protocol，中文名叫做超文本传输协议，HTTP 协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证传送高效而准确地传送超文本文档。HTTP 由万维网协会（World Wide Web Consortium）和 Internet 工作小组IETF（Internet Engineering Task Force）共同合作制定的规范，目前广泛使用的是 HTTP 1.1 版本。</p>
<p>HTTPS 的全称是 Hyper Text Transfer Protocol over Secure Socket Layer，是以安全为目标的 HTTP 通道，简单讲是 HTTP 的安全版，即 HTTP 下加入 SSL 层，简称为 HTTPS。</p>
<p>HTTPS 的安全基础是 SSL，因此通过它传输的内容都是经过 SSL 加密的，它的主要作用可以分为两种：</p>
<ul>
<li>是建立一个信息安全通道，来保证数据传输的安全。</li>
<li>确认网站的真实性，凡是使用了 https 的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过 CA 机构颁发的安全签章来查询。</li>
</ul>
<p>现在越来越多的网站和 APP 都已经向 HTTPS 方向发展。例如：</p>
<ul>
<li>苹果公司强制所有 iOS App 在 2017 年 1 月 1 日 前全部改为使用 HTTPS 加密，否则 APP 就无法在应用商店上架。</li>
<li>谷歌从 2017 年 1 月推出的 Chrome 56 开始，对未进行 HTTPS 加密的网址链接亮出风险提示，即在地址栏的显著位置提醒用户“此网页不安全”。</li>
<li>腾讯微信小程序的官方需求文档要求后台使用 HTTPS 请求进行网络通信，不满足条件的域名和协议无法请求。</li>
</ul>
<p>而某些网站虽然使用了 HTTPS 协议还是会被浏览器提示不安全，例如我们在 Chrome 浏览器里面打开 12306，链接为：<a href="https://www.12306.cn/" target="_blank" rel="noopener">https://www.12306.cn/</a>，这时浏览器就会提示“您的连接不是私密连接”这样的话，如图 2-3 所示：</p>
<p><img src="https://germey.gitbooks.io/python3webspider/content/assets/2-3.png" alt="img"></p>
<p>图 2-3 12306 页面</p>
<p>这是因为 12306 的 CA 证书是中国铁道部自己颁发给自己的，而这个证书是不被官方机构认可的，所以这里证书验证就不会通过而提示这样的话，但是实际上它的数据传输依然是经过 SSL 加密的。我们如果要爬取这样的站点就需要设置忽略证书的选项，否则会提示 SSL 链接错误，在后文会进行详细说明。</p>
<h3 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h3><p>Request， 即请求， 由客户端向服务端发出， 可以将Request划分为四部分内容: Request Method, Request URL, Request Headers, Request Body, 即请求方式， 请求链接， 请求头， 请求体。</p>
<h3 id="Request-Method"><a href="#Request-Method" class="headerlink" title="Request Method"></a>Request Method</h3><p>请求方式， 请求方式常见的由两种类型， GET和POST。</p>
<p>GET和POST请求方法有如下区别：</p>
<ol>
<li>GET方式请求中参数是包含在URL里面的， 数据可以在URL中看到， 而POST请求的URL不会包含这些数据， 数据都是通过表单的形式传输， 会包含在Request Body中。</li>
<li>GET方式请求提交的数据最多只有1024字节， 而POST方式没有限制。</li>
</ol>
<p>所以一般来说，网站登录验证的时候，需要提交用户名密码，这里包含了敏感信息，使用GET方式请求的话密码就会暴露在URL里面，造成密码泄露，所以这里最好以POST方式发送。文件的上传时，由于文件内容比较大，也会选用POST方式。</p>
<p>我们平常遇到的绝大部分请求都是 GET 或 POST 请求，另外还有一些请求方式，如 HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE，我们简单将其总结如下：</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>GET</td>
<td>请求指定的页面信息，并返回实体主体。</td>
</tr>
<tr>
<td>HEAD</td>
<td>类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头。</td>
</tr>
<tr>
<td>POST</td>
<td>向指定资源提交数据进行处理请求，数据被包含在请求体中。</td>
</tr>
<tr>
<td>PUT</td>
<td>从客户端向服务器传送的数据取代指定的文档的内容。</td>
</tr>
<tr>
<td>DELETE</td>
<td>请求服务器删除指定的页面。</td>
</tr>
<tr>
<td>CONNECT</td>
<td>HTTP/1.1 协议中预留给能够将连接改为管道方式的代理服务器。</td>
</tr>
<tr>
<td>OPTIONS</td>
<td>允许客户端查看服务器的性能。</td>
</tr>
<tr>
<td>TRACE</td>
<td>回显服务器收到的请求，主要用于测试或诊断。</td>
</tr>
</tbody>
</table>
<h3 id="Request-URL"><a href="#Request-URL" class="headerlink" title="Request URL"></a>Request URL</h3><p>顾名思义， 就是请求的网址， 即统一资源定位符， 用URL可以唯一确定我们想请求的资源。</p>
<h3 id="Request-Headers"><a href="#Request-Headers" class="headerlink" title="Request Headers"></a>Request Headers</h3><p>请求头， 用来说明服务器要使用的附加信息， 比较重要的信息有Cookie， Referer, User-Agent 等， 下面将一些常用的头信息说明如下：</p>
<ul>
<li>Accept， 请求报头域， 用于指定客户端可以接受哪些类型的信息。</li>
<li>Accept-Language， 指定客户端可以接受的语言类型。</li>
<li>Accept-Encoding, 指定客户端可接受的内容编码。</li>
<li>Host， 用于指定请求资源的主机IP和端口号， 其内容为请求URL的原始服务器或网关的位置。 从HTTP1.1 版本开始， Request必须包含此内容。</li>
<li>Cookie， 也常用复数形式Cookies, 是网站为了辨别用户进行Session跟踪而储存在用户本地的数据。Cookie的主要功能就是维持当前访问会话， 例如我们输入用户名密码登陆了某个网站， 登陆成功之后服务器会用Session保存我们的登陆状态信息， 后来我们每次刷新或请求该站点的其他页面时会发现都是保持着登陆状态的， 在这里就是Cookies的功劳， Cookies里有信息标识了我们所对应的服务器的Session绘画， 每次浏览器在请求该站点的服务时都会在请求头中加上Cookies并将其发送给服务器， 服务通过Cookies识别出是我们自己， 并且查出当前状态是登陆的状态， 所以返回的结果就是登陆之后才能看到的网页内容。</li>
<li>Referer, 此内容用来标识这个请求是从哪个页面发过来的， 服务器可以拿到这一信息并做相应的处理，如做来源统计， 做防盗链处理等。</li>
<li>User-Agent, 简称UA， 它是一个特殊字符串头， 使得服务器能够识别客户使用的操作系统以及版本， 浏览器及版本等信息。 在做爬虫时加上此信息可以伪装成浏览器，如果不加可能会被识别出为爬虫。</li>
<li>Content-Type， 即Internet Media Type， 互联网媒体类型， 也叫做MIME类型， 在HTTP协议消息头中， 使用它来表示具体请求中的媒体类型信息。 例如 text/html 代表 HTML 格式，image/gif 代表 GIF 图片，application/json 代表 Json 类型等。</li>
</ul>
<p>因此，Request Headers 是 Request 等重要组成部分，在写爬虫的时候大部分情况都需要设定 Request Headers。</p>
<h3 id="Request-Body"><a href="#Request-Body" class="headerlink" title="Request Body"></a>Request Body</h3><p>即请求体，一般承载的内容是 POST 请求中的 Form Data，即表单数据，而对于 GET 请求 Request Body 则为空。</p>
<p>例如在这里我登录 GitHub 时捕获到的 Request 和 Response 如图 2-8 所示：</p>
<p><img src="https://germey.gitbooks.io/python3webspider/content/assets/2-8.jpg" alt="img"></p>
<p>在登录之前我们填写了用户名和密码信息，提交时就这些内容就会以 Form Data 的形式提交给服务器，此时注意 Request Headers 中指定了 Content-Type 为 application/x-www-form-urlencoded，只有设置 Content-Type 为 application/x-www-form-urlencoded 才会以 Form Data 形式提交，另外我们也可以将 Content-Type 设置为 application/json 来提交 Json 数据，或者设置为 multipart/form-data 来上传文件。</p>
<p>下面列出了 Content-Type 和 POST 提交数据方式的关系：</p>
<table>
<thead>
<tr>
<th>Content-Type</th>
<th>提交数据方式</th>
</tr>
</thead>
<tbody>
<tr>
<td>application/x-www-form-urlencoded</td>
<td>Form 表单提交</td>
</tr>
<tr>
<td>multipart/form-data</td>
<td>表单文件上传提交</td>
</tr>
<tr>
<td>application/json</td>
<td>序列化 Json 数据提交</td>
</tr>
<tr>
<td>text/xml</td>
<td>XML 数据提交</td>
</tr>
</tbody>
</table>
<p>在爬虫中如果我们要构造 POST 请求需要注意这几种 Content-Type，了解各种请求库的各个参数设置时使用的是哪种 Content-Type，不然可能会导致 POST 提交后得不到正常的 Response。</p>
<p>以上便是对 Request 各部分内容的解释。</p>
<h3 id="Response"><a href="#Response" class="headerlink" title="Response"></a>Response</h3><p>Response， 即响应， 由服务器端返回给客户端。 Response可以划分为三部分， Respond status Code, Response Headers, Response Body.</p>
<h3 id="Respond-Status-Code"><a href="#Respond-Status-Code" class="headerlink" title="Respond Status Code"></a>Respond Status Code</h3><p>响应状态码，此状态码表示了服务器的响应状态，如 200 则代表服务器正常响应，404 则代表页面未找到，500 则代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如判断状态码为 200，则证明成功返回数据，再进行进一步的处理，否则直接忽略。</p>
<p>下面用表格列出了常见的错误代码及错误原因：</p>
<table>
<thead>
<tr>
<th>状态码</th>
<th>说明</th>
<th>详情</th>
</tr>
</thead>
<tbody>
<tr>
<td>100</td>
<td>继续</td>
<td>请求者应当继续提出请求。服务器已收到请求的一部分，正在等待其余部分。</td>
</tr>
<tr>
<td>101</td>
<td>切换协议</td>
<td>请求者已要求服务器切换协议，服务器已确认并准备切换。</td>
</tr>
<tr>
<td>200</td>
<td>成功</td>
<td>服务器已成功处理了请求。</td>
</tr>
<tr>
<td>201</td>
<td>已创建</td>
<td>请求成功并且服务器创建了新的资源。</td>
</tr>
<tr>
<td>202</td>
<td>已接受</td>
<td>服务器已接受请求，但尚未处理。</td>
</tr>
<tr>
<td>203</td>
<td>非授权信息</td>
<td>服务器已成功处理了请求，但返回的信息可能来自另一来源。</td>
</tr>
<tr>
<td>204</td>
<td>无内容</td>
<td>服务器成功处理了请求，但没有返回任何内容。</td>
</tr>
<tr>
<td>205</td>
<td>重置内容</td>
<td>服务器成功处理了请求，内容被重置。</td>
</tr>
<tr>
<td>206</td>
<td>部分内容</td>
<td>服务器成功处理了部分请求。</td>
</tr>
<tr>
<td>300</td>
<td>多种选择</td>
<td>针对请求，服务器可执行多种操作。</td>
</tr>
<tr>
<td>301</td>
<td>永久移动</td>
<td>请求的网页已永久移动到新位置，即永久重定向。</td>
</tr>
<tr>
<td>302</td>
<td>临时移动</td>
<td>请求的网页暂时跳转到其他页面，即暂时重定向。</td>
</tr>
<tr>
<td>303</td>
<td>查看其他位置</td>
<td>如果原来的请求是 POST，重定向目标文档应该通过 GET 提取。</td>
</tr>
<tr>
<td>304</td>
<td>未修改</td>
<td>此次请求返回的网页未修改，继续使用上次的资源。</td>
</tr>
<tr>
<td>305</td>
<td>使用代理</td>
<td>请求者应该使用代理访问该网页。</td>
</tr>
<tr>
<td>307</td>
<td>临时重定向</td>
<td>请求的资源临时从其他位置响应。</td>
</tr>
<tr>
<td>400</td>
<td>错误请求</td>
<td>服务器无法解析该请求。</td>
</tr>
<tr>
<td>401</td>
<td>未授权</td>
<td>请求没有进行身份验证或验证未通过。</td>
</tr>
<tr>
<td>403</td>
<td>禁止访问</td>
<td>服务器拒绝此请求。</td>
</tr>
<tr>
<td>404</td>
<td>未找到</td>
<td>服务器找不到请求的网页。</td>
</tr>
<tr>
<td>405</td>
<td>方法禁用</td>
<td>服务器禁用了请求中指定的方法。</td>
</tr>
<tr>
<td>406</td>
<td>不接受</td>
<td>无法使用请求的内容响应请求的网页。</td>
</tr>
<tr>
<td>407</td>
<td>需要代理授权</td>
<td>请求者需要使用代理授权。</td>
</tr>
<tr>
<td>408</td>
<td>请求超时</td>
<td>服务器请求超时。</td>
</tr>
<tr>
<td>409</td>
<td>冲突</td>
<td>服务器在完成请求时发生冲突。</td>
</tr>
<tr>
<td>410</td>
<td>已删除</td>
<td>请求的资源已永久删除。</td>
</tr>
<tr>
<td>411</td>
<td>需要有效长度</td>
<td>服务器不接受不含有效内容长度标头字段的请求。</td>
</tr>
<tr>
<td>412</td>
<td>未满足前提条件</td>
<td>服务器未满足请求者在请求中设置的其中一个前提条件。</td>
</tr>
<tr>
<td>413</td>
<td>请求实体过大</td>
<td>请求实体过大，超出服务器的处理能力。</td>
</tr>
<tr>
<td>414</td>
<td>请求 URI 过长</td>
<td>请求网址过长，服务器无法处理。</td>
</tr>
<tr>
<td>415</td>
<td>不支持类型</td>
<td>请求的格式不受请求页面的支持。</td>
</tr>
<tr>
<td>416</td>
<td>请求范围不符</td>
<td>页面无法提供请求的范围。</td>
</tr>
<tr>
<td>417</td>
<td>未满足期望值</td>
<td>服务器未满足期望请求标头字段的要求。</td>
</tr>
<tr>
<td>500</td>
<td>服务器内部错误</td>
<td>服务器遇到错误，无法完成请求。</td>
</tr>
<tr>
<td>501</td>
<td>未实现</td>
<td>服务器不具备完成请求的功能。</td>
</tr>
<tr>
<td>502</td>
<td>错误网关</td>
<td>服务器作为网关或代理，从上游服务器收到无效响应。</td>
</tr>
<tr>
<td>503</td>
<td>服务不可用</td>
<td>服务器目前无法使用。</td>
</tr>
<tr>
<td>504</td>
<td>网关超时</td>
<td>服务器作为网关或代理，但是没有及时从上游服务器收到请求。</td>
</tr>
<tr>
<td>505</td>
<td>HTTP 版本不支持</td>
<td>服务器不支持请求中所用的 HTTP 协议版本。</td>
</tr>
</tbody>
</table>
<h3 id="Respond-Headers"><a href="#Respond-Headers" class="headerlink" title="Respond Headers"></a>Respond Headers</h3><p>响应头， 其中包含了服务器对请求的应答信息，如Content-Type， Server, Set-Cookie等， 下面将一些常用的头信息说明如下：</p>
<ul>
<li>Data， 标识Response产生的时间。</li>
<li>Last-Modified, 指定资源的最后修改时间。</li>
<li>Content-Encoding， 指定Response内容的编码。</li>
<li>Server， 包含了服务器的信息， 名称，版本号等。</li>
<li>Content-Type, 文档类型， 指定了返回的数据类型是什么， 如text/html。</li>
<li>Set-Cookie, 设置Cookie， Response Headers中的Set-Cookie即告诉浏览器需要将此内容放在Cookies中， 下次请求携带Cookies请求。</li>
<li>Expires， 指定Response的过期时间， 使用它可以控制代理服务器或浏览器将内容更新到缓存中， 如果再次访问时， 直接从缓存中加载， 降低服务器负载， 缩短加载时间。</li>
</ul>
<h3 id="Response-Body"><a href="#Response-Body" class="headerlink" title="Response Body"></a>Response Body</h3><p>即响应体，最重要的当属响应体内容了，响应的正文数据都是在响应体中，如请求一个网页，它的响应体就是网页的 HTML 代码，请求一张图片，它的响应体就是图片的二进制数据。所以最主要的数据都包含在响应体中了，我们做爬虫请求网页后要解析的内容就是解析响应体，如图 2-9 所示：<img src="https://germey.gitbooks.io/python3webspider/content/assets/2-9.jpg" alt="img"></p>
<p>我们在浏览器开发者工具中点击 Preview，就可以看到网页的源代码，这也就是响应体内容，是解析的目标。</p>
<p>我们在做爬虫时主要解析的内容就是 Resposne Body，通过 Resposne Body 我们可以得到网页的源代码、Json 数据等等，然后从中做相应内容的提取。</p>
<p>以上便是 Response 的组成部分。</p>
<h2 id="爬虫基本原理"><a href="#爬虫基本原理" class="headerlink" title="爬虫基本原理"></a>爬虫基本原理</h2><p>简单来说， 爬虫就是获取网页并提取和保存信息的自动化程序， 接下来对各个点进行说明：</p>
<h3 id="获取网页"><a href="#获取网页" class="headerlink" title="获取网页"></a>获取网页</h3><p>爬虫首先要做到的工作就是获取网页， 在这里获取网页即获取网页的源代码， 源代码里面必然包含了网页的部分有用的信息， 所以只要把源代码获取下来了， 就可以从中提取我们想要的信息了。</p>
<p>在前面我们降到了Request和Response的概念， 我们向网站的服务器发送一个Request， 返回的Response的Body就是网页源代码， 所以最关键的部分就是构造一个Request并发送给服务器， 然后接收到Response并将其解析出来。</p>
<h3 id="提取信息"><a href="#提取信息" class="headerlink" title="提取信息"></a>提取信息</h3><p>我们在第一步获取了网页源代码之后，接下来的工作就是分析网页源代码，从中提取我们想要的数据，首先最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式的时候比较复杂且容易出错。</p>
<p>另外由于网页的结构是有一定规则的，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 BeautifulSoup、PyQuery、LXML 等，使用这些库可以高效快速地从中提取网页信息，如节点的属性、文本值等内容。</p>
<p>提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得清晰条理，以便于我们后续在对数据进行处理和分析。</p>
<h3 id="保存信息"><a href="#保存信息" class="headerlink" title="保存信息"></a>保存信息</h3><p>提取信息之后我们一般会将提取到的数据保存到某处以便后续数据处理使用。保存形式有多种多样，如可以简单保存为 TXT 文本或 Json 文本，也可以保存到数据库，如 MySQL、MongoDB 等，也可保存至远程服务器，如借助 Sftp 进行操作等。</p>
<h3 id="能抓怎么的数据"><a href="#能抓怎么的数据" class="headerlink" title="能抓怎么的数据"></a>能抓怎么的数据</h3><p>在网页中我们能看到各种各样的信息，最常见的便是常规网页，其都对应着 HTML 代码，而最常见的抓取便是抓取 HTML 源代码。</p>
<p>另外可能有些网页返回的不是 HTML 代码，而是返回一个 Json 字符串，API 接口大多采用这样的形式，方便数据的传输和解析，这种数据同样可以抓取，而且数据提取更加方便。</p>
<p>此外我们还可以看到各种二进制数据，如图片、视频、音频等等，我们可以利用爬虫将它们的二进制数据抓取下来，然后保存成对应的文件名即可。</p>
<p>另外我们还可以看到各种扩展名的文件，如 CSS、JavaScript、配置文件等等，这些其实也是最普通的文件，只要在浏览器里面访问到，我们就可以将其抓取下来。</p>
<p>以上的内容其实都对应着各自的URL，是基于 HTTP 或 HTTPS 协议的，只要是这种数据爬虫都可以进行抓取。</p>
<h3 id="JavaScript渲染页面"><a href="#JavaScript渲染页面" class="headerlink" title="JavaScript渲染页面"></a>JavaScript渲染页面</h3><p>有时候我们用Urllib或Requests抓取网页时， 得到的源代码实际和浏览器中看到的是不一样的。</p>
<p>这个问题是一个非常常见的问题， 现在网页越来越多地采用Ajax， 前端模块化工具来构建网页， 整个网页可能都是由JavaScript渲染出来的， 意思就是说原始的HTML代码就是一个空壳， 例如：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE html&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">"UTF-8"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>This is a Demo<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"app.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>body 节点里面只有一个 id 为 container 的节点，但是注意到在 body 节点后引入了一个 app.js，这个便负责了整个网站的渲染。</p>
<p>在浏览器打开这个页面时，首先会加载这个 HTML 内容，接着浏览器会发现其中里面引入了一个 app.js 文件，然后浏览器便会接着去请求这个文件，获取到该文件之后便会执行其中的 JavaScript 代码，而 JavaScript 则会改变 HTML 中的节点，向内添加内容，最后得到完整的页面。</p>
<p>但是在用 Urllib 或 Requests 等库来请求当前页面时，我们得到的只是这个 HTML 代码，它不会帮助我们去继续加载这个 JavaScript 文件，这样也就看不到浏览器中看到的内容了。</p>
<p>这也解释了为什么有时我们得到的源代码和浏览器中看到的是不一样的。</p>
<p>所以使用基本 HTTP 请求库得到的结果源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，我们可以分析其后台 Ajax 接口，也可使用 Selenium、Splash 这样的库来实现模拟 JavaScript 渲染，这样我们便可以爬取 JavaScript 渲染的网页的内容了。</p>
<p>在后文我们会详细介绍对于 JavaScript 渲染的网页的采集方法。</p>
<h3 id="登陆功能的实现"><a href="#登陆功能的实现" class="headerlink" title="登陆功能的实现"></a>登陆功能的实现</h3><p>动态网站还可以实现用户登陆注册的功能， 很多页面是通过登陆之后才可以查看的， 按照一般的逻辑来说， 我们输入了用户名密码登陆之后， 肯定是拿到了一种类似凭证的东西， 有了它我们才能保证登陆状态， 才能访问登陆之后才能看到的页面。</p>
<p>那么这种神秘的凭证到底是什么呢？ 其实它就是Session和Cookies共同产生的结果。</p>
<p>在了解 Session 和 Cookies 之前，我们还需要了解 HTTP 的一个特点，叫做无状态。</p>
<p>HTTP 的无状态是指 HTTP 协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。当我们向服务器发送一个 Requset 后，服务器解析此 Request，然后返回对应的 Response，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录，这意味着如果后续需要处理需要前面的信息，则它必须要重传，这也导致了需要额外传递一些前面的重复 Request 才能获取后续 Response，然而这种效果显然不是我们想要的。为了保持前后状态，我们肯定不能将前面的请求全部重传一次，这太浪费资源了，对于这种需要用户登录的页面来说，更是棘手。</p>
<p>所以，这时候，两个用于保持 HTTP 连接状态的技术就出现了，它们分别是 Session 和 Cookies，Session 在服务端，也就是网站的服务器，用来保存用户的会话信息，Cookies 在客户端，也可以理解为浏览器端，有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的 Response。</p>
<p>所以我们可以理解为 Cookies 里面保存了登录的凭证，有了它我们只需要在下次请求携带 Cookies 发送 Request 而不必重新输入用户名、密码等信息重新登录了。</p>
<p>因此在爬虫中，有时候处理需要登录才能访问的页面时，我们一般会直接将登录成功后获取的 Cookies 放在 Request Headers 里面直接请求，而不必重新模拟登录。</p>
<p>好，大体了解什么是 Session 和 Cookies 之后，我们来详细剖析一下它们的原理。</p>
<h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>Session，即会话，其本来的含义是指有始有终的一系列动作/消息，比如打电话时从拿起电话拨号到挂断电话这中间的一系列过程可以称之为一个 Session。</p>
<p>而在 Web 中 Session 对象用来存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当用户请求来自应用程序的 Web 页时，如果该用户还没有会话，则 Web 服务器将自动创建一个 Session 对象。当会话过期或被放弃后，服务器将终止该会话。</p>
<h4 id="Cookies"><a href="#Cookies" class="headerlink" title="Cookies"></a>Cookies</h4><p>Cookie，有时也用其复数形式 Cookies，指某些网站为了辨别用户身份、进行 Session 跟踪而储存在用户本地终端上的数据。</p>
<p>那么利用 Cookies 我们是怎样来保持状态的呢？当客户端第一次请求服务器时，服务器会返回一个 Headers 中带有 Set-Cookie 字段的 Response 给客户端，用来标记是哪一个用户，客户端浏览器会把Cookies 保存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies 放到 Request Headers 一起提交给服务器，Cookies 携带了 Session ID 信息，服务器检查该 Cookies 即可找到对应的 Session 是什么，然后再判断 Session 来以此来辨认用户状态。</p>
<p>所以我们在登录某个网站的时候，登录成功后服务器会告诉客户端设置哪些 Cookies 信息，在后续访问页面时客户端会把 Cookies 发送给服务器，服务器再找到对应的 Session 加以判断，如果 Session 中的某些设置登录状态的变量是有效的，那就证明用户是处于登录状态的，即可返回登录之后才可以查看的网页内容，浏览器进行解析便可以看到了。</p>
<p>反之，如果传给服务器的 Cookies 是无效的，或者 Session 已经过期了，我们将不能继续访问页面，可能会收到错误的 Response 或者跳转到登录页面重新登录。</p>
<p>所以 Cookies 和 Session 需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录会话控制。</p>
<h3 id="Cookies-基本结构"><a href="#Cookies-基本结构" class="headerlink" title="Cookies 基本结构"></a>Cookies 基本结构</h3><p>我们可以看到 Cookies 有一个个条目，每个条目我们可以称之为 Cookie，取单数形式。它有这么几个属性：</p>
<ul>
<li>Name，即该 Cookie 的名称。Cookie 一旦创建，名称便不可更改</li>
<li>Value，即该 Cookie 的值。如果值为 Unicode 字符，需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码。</li>
<li>Max Age，即该 Cookie 失效的时间，单位秒，也常和 Expires 一起使用，通过它可以计算出其有效时间。Max Age 如果为正数，则该Cookie 在 Max Age 秒之后失效。如果为负数，则关闭浏览器时Cookie 即失效，浏览器也不会以任何形式保存该 Cookie。 也就是会话cookie与持久cookie的区别。</li>
<li>Path，即该 Cookie 的使用路径。如果设置为 /path/，则只有路径为 /path/ 的页面可以访问该 Cookie。如果设置为 /，则本域名下的所有页面都可以访问该 Cookie。</li>
<li>Domain，即可以访问该 Cookie 的域名。例如如果设置为 .zhihu.com，则所有以 zhihu.com，结尾的域名都可以访问该Cookie。</li>
<li>Size字段，即此 Cookie 的大小。</li>
<li>Http字段，即 Cookie 的 httponly 属性。若此属性为 true，则只有在 HTTP Headers 中会带有此 Cookie 的信息，而不能通过 document.cookie 来访问此 Cookie。</li>
<li>Secure，即该 Cookie 是否仅被使用安全协议传输。安全协议。安全协议有 HTTPS，SSL 等，在网络上传输数据之前先将数据加密。默认为 false。</li>
</ul>
<p>以上便是 Cookies 的基本结构。</p>
<h3 id="常见误区"><a href="#常见误区" class="headerlink" title="常见误区"></a>常见误区</h3><p>在讨论Session机制的时候， 常常听到这样一种误解， 只要关闭浏览器，Session就消失了， 这中理解是错误的。 除非程序通知服务器删除一个Session， 否则服务器会一直保留， 比如程序一般都是在我们做注销操作的时候采取删除Session。</p>
<p>但是当我们关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭，之所以会有这种错觉，是大部分 Session 机制都使用会话 Cookie 来保存 Session ID 信息，而关闭浏览器后 Cookies 就消失了，再次连接服务器时也就无法找到原来的 Session。如果服务器设置的 Cookies 被保存到硬盘上，或者使用某种手段改写浏览器发出的 HTTP 请求头，把原来的 Cookies 发送给服务器，则再次打开浏览器仍然能够找到原来的 Session ID，依旧还是可以保持登录状态的。</p>
<p>恰恰是由于关闭浏览器不会导致Session被删除， 这就需要服务器为Session设置一个失效时间， 当距离客户端上一次使用Session的时间超过这个失效时间时， 服务器就可以认为客户端已经停止了活动， 才会把Session删除以节省存储空间。</p>
<h2 id="代理基本原理"><a href="#代理基本原理" class="headerlink" title="代理基本原理"></a>代理基本原理</h2><p>我们在做爬虫的过程中经常会遇到这样的情况，最初爬虫正常运行，正常抓取数据，一切看起来都是那么的美好，然而一杯茶的功夫可能就会出现错误，比如 403 Forbidden，这时候打开网页一看，可能会看到“您的 IP 访问频率太高”这样的提示。出现这样的现象的原因是网站采取了一些反爬虫的措施，比如服务器会检测某个 IP 在单位时间内的请求次数，如果超过了这个阈值，那么会直接拒绝服务，返回一些错误信息，这种情况可以称之为封 IP，于是乎就成功把我们的爬虫禁掉了。</p>
<p>既然服务器检测的是某个 IP 单位时间的请求次数，那么我们借助某种方式来伪装我们的 IP，让服务器识别不出是由我们本机发起的请求，不就可以成功防止封 IP 了吗？</p>
<p>那么在这里一种有效的方式就是使用代理，使用它我们可以成功伪装 IP，避免本机 IP 被封禁的情况，在后文会有详细的代理使用的说明，在这之前我们需要先了解下代理的基本原理，它是怎样实现 IP 伪装的呢？本节就让我们先了解一下代理的概念。</p>
<h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>我们常称呼的代理实际上就是代理服务器， 英文叫做Proxy Server， 它的功能是代理网络用户去取得网络信息。 形象地说， 它是网络信息的中转站。 在我们正常请求一个网站时， 是发送了Request给Web服务器， Web服务器把Response传给我们， 如果设置了代理服务器，实际上就是在本机和服务器之间搭建了一个桥，此时本机不是直接向 Web 服务器发起请求，而是向代理服务器发出请求， Request 会发送给代理服务器，然后由代理服务器再发送给 Web 服务器，然后由代理服务器再把 Web 服务器返回的 Response 转发给本机，这样我们同样可以正常访问网页，但这个过程 Web 服务器识别出的真实的 IP 就不再是我们本机的 IP 了，就成功实现了 IP 伪装，这就是代理的基本原理。</p>
<h3 id="爬虫代理"><a href="#爬虫代理" class="headerlink" title="爬虫代理"></a>爬虫代理</h3><p>对于爬虫来说，由于爬虫爬取速度过快，在爬取过程中可能遇到同一个 IP 访问过于频繁的问题，网站就会让我们输入验证码或登录或者直接封锁 IP，这样会给爬取带来极大的不便。</p>
<p>所以使用代理隐藏真实的 IP，让服务器误以为是代理服务器的在请求自己。这样在爬取过程中通过不断更换代理，就不会被封锁，可以达到很好的爬取效果。</p>
<h2 id="Urllib简介"><a href="#Urllib简介" class="headerlink" title="Urllib简介"></a>Urllib简介</h2><p>我们首先了解以下Urllib库， 它是Python内置的HTTP请求库。 它包含四个模块：</p>
<ul>
<li>第一个模块 request， 它是最基本的HTTP请求模块， 我们可以用它来模拟发送一请求， 就像在浏览器里输入网址然后敲击回车一样， 只需要给库方法传入URL还有额外的参数， 就可以模拟实现这个过程了。</li>
<li>第二个error模块即异常处理模块， 如果出现请求错误， 我们可以捕获这些异常， 然后进行重试或者其他操作程序保证程序不会意外终止。</li>
<li>第三个parse 模块是一个工具模块， 提供了许多URL处理方法， 比如拆分， 解析， 合并等等的方法。</li>
<li>第四个模块是robotparser， 主要是用来识别网站的robots.txt文件， 然后判断哪些网站可以爬， 哪些网站不可以爬。</li>
</ul>
<h3 id="Request-1"><a href="#Request-1" class="headerlink" title="Request"></a>Request</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=<span class="keyword">None</span>, [timeout, ]*, cafile=<span class="keyword">None</span>, capath=<span class="keyword">None</span>, cadefault=<span class="keyword">False</span>, context=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure>
<p>我们首先看一下urlopen()函数的API， 可以发现除了第一个参数可以传递URL之外， 我们还可以传递其它的内容， 比如data， timeout等等。</p>
<h4 id="Data参数"><a href="#Data参数" class="headerlink" title="Data参数"></a>Data参数</h4><p>data参数是可选的， 如果要添加data， 它要是字节流编码格式的内容， 即bytes类型， 通过bytes()方法可以进行转化， 另外如果传递了这个data参数， 它的请求方式就不再是GET方式请求， 而是POST。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'word'</span>: <span class="string">'hello'</span>&#125;), encoding=<span class="string">'utf8'</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">'http://httpbin.org/post'</span>, data=data)</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure>
<h4 id="Time-out-参数"><a href="#Time-out-参数" class="headerlink" title="Time out 参数"></a>Time out 参数</h4><h4 id="Context-参数"><a href="#Context-参数" class="headerlink" title="Context 参数"></a>Context 参数</h4><p>它必须是ssl.SSLContext类型， 用来指定SSL设置</p>
<p>cafile和capath两个参数是指定CA证书和它的路径， 这个在请求HTTPS链接时会有用。</p>
<h3 id="Request类的使用"><a href="#Request类的使用" class="headerlink" title="Request类的使用"></a>Request类的使用</h3><p>由上我们知道利用 urlopen() 方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入 Headers 等信息，我们就可以利用更强大的 Request 类来构建一个请求。</p>
<p>我们依然是用urlopen()方法来发送这个请求， 只不过这次urlopen()方法的参数不再是一个URL， 而是一个Request类型的对象， 通过构造这个数据结构， 一方面我们可以将请求独立成一个对象， 另一方面可配置参数更加丰富和灵活。</p>
<p>request构造方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(url, data=None, headers=&#123;&#125;, origin_req_host=None, unverifiable=False, method=None)</span></span></span><br></pre></td></tr></table></figure>
<p>第一个 url 参数是请求 URL，这个是必传参数，其他的都是可选参数。</p>
<p>第二个 data 参数如果要传必须传 bytes（字节流）类型的，如果是一个字典，可以先用 urllib.parse 模块里的 urlencode() 编码。</p>
<p>第三个 headers 参数是一个字典，这个就是 Request Headers 了，你可以在构造 Request 时通过 headers 参数直接构造，也可以通过调用 Request 实例的 add_header() 方法来添加。</p>
<p>添加 Request Headers 最常用的用法就是通过修改 User-Agent 来伪装浏览器，默认的 User-Agent 是 Python-urllib，我们可以通过修改它来伪装浏览器，比如要伪装火狐浏览器，你可以把它设置为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mozilla/<span class="number">5.0</span> (X11; U; Linux i686) Gecko/<span class="number">20071127</span> Firefox/<span class="number">2.0</span><span class="number">.0</span><span class="number">.11</span></span><br></pre></td></tr></table></figure>
<p>第四个 origin_req_host 参数指的是请求方的 host 名称或者 IP 地址。</p>
<p>第五个 unverifiable 参数指的是这个请求是否是无法验证的，默认是False。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个 HTML 文档中的图片，但是我们没有自动抓取图像的权限，这时 unverifiable 的值就是 True。</p>
<p>第六个 method 参数是一个字符串，它用来指示请求使用的方法，比如GET，POST，PUT等等。</p>
<p>下面我们传入多个参数构建一个Request来感受以下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from urllib import request, parse</span><br><span class="line"></span><br><span class="line">url = &apos;http://httpbin.org/post&apos;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos;,</span><br><span class="line">    &apos;Host&apos;: &apos;httpbin.org&apos;</span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;</span><br><span class="line">    &apos;name&apos;: &apos;Germey&apos;</span><br><span class="line">&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict), encoding=&apos;utf8&apos;)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=&apos;POST&apos;)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(&apos;utf-8&apos;))</span><br></pre></td></tr></table></figure>
<p>在这里我们通过四个参数构造了一个Request， url即请求URL， 在headers中指定了User-Agent和Host， 传递的参数用了urlencode()和bytes()方法来转成字节流， 另外指定了请求方式为POST。</p>
<h3 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h3><p>在上面的过程中， 我们虽然可以构造Request, 但是一些更高级的操作， 比如Cookies处理， 代理设置等无法进行。</p>
<p>接下来就需要更强大的工具 Handler 登场了。</p>
<p>简而言之我们可以把它理解为各种处理器，有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的，利用它们我们几乎可以做到任何 HTTP 请求中所有的事情。</p>
<p>首先介绍下 urllib.request 模块里的 BaseHandler类，它是所有其他 Handler 的父类，它提供了最基本的 Handler 的方法，例如 default_open()、protocol_request() 方法等。</p>
<p>接下来就有各种 Handler 子类继承这个 BaseHandler 类，举例几个如下：</p>
<ul>
<li>HTTPDefaultErrorHandler 用于处理 HTTP 响应错误，错误都会抛出 HTTPError 类型的异常。</li>
<li>HTTPRedirectHandler 用于处理重定向。</li>
<li>HTTPCookieProcessor 用于处理 Cookies。</li>
<li>ProxyHandler 用于设置代理，默认代理为空。</li>
<li>HTTPPasswordMgr 用于管理密码，它维护了用户名密码的表。</li>
<li>HTTPBasicAuthHandler 用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。</li>
<li>另外还有其他的 Handler 类，在这不一一列举了，详情可以参考官方文档： <a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="noopener">https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler</a></li>
</ul>
<p>另外一个比较重要的类就是 OpenerDirector，我们可以称之为 Opener，我们之前用过 urlopen() 这个方法，实际上它就是 Urllib为我们提供的一个 Opener。</p>
<p>那么为什么要引入 Opener 呢？因为我们需要实现更高级的功能，之前我们使用的 Request、urlopen() 相当于类库为你封装好了极其常用的请求方法，利用它们两个我们就可以完成基本的请求，但是现在不一样了，我们需要实现更高级的功能，所以我们需要深入一层进行配置，使用更底层的实例来完成我们的操作。</p>
<p>所以，在这里我们就用到了比调用 urlopen() 的对象的更普遍的对象，也就是 Opener。</p>
<p>Opener 可以使用 open() 方法，返回的类型和 urlopen() 如出一辙。那么它和 Handler 有什么关系？简而言之，就是利用 Handler 来构建 Opener。</p>
<p>下面我们用几个实例来感受一下他们的用法：</p>
<h4 id="认证"><a href="#认证" class="headerlink" title="认证"></a>认证</h4><p>有些网站在打开时它就弹出了一个框，直接提示你输入用户名和密码，认证成功之后才能查看页面，如图 3-2 所示：</p>
<p><img src="https://germey.gitbooks.io/python3webspider/content/assets/3-2.jpg" alt="img"></p>
<p>那么我们如果要请求这样的页面怎么办呢？</p>
<p>借助于 HTTPBasicAuthHandler 就可以完成，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line">from urllib.error import URLError</span><br><span class="line"></span><br><span class="line">username = &apos;username&apos;</span><br><span class="line">password = &apos;password&apos;</span><br><span class="line">url = &apos;http://localhost:5000/&apos;</span><br><span class="line"></span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(None, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(&apos;utf-8&apos;)</span><br><span class="line">    print(html)</span><br><span class="line">except URLError as e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>在这里，首先实例化了一个 HTTPBasicAuthHandler 对象，参数是 HTTPPasswordMgrWithDefaultRealm 对象，它利用 add_password() 添加进去用户名和密码，这样我们就建立了一个处理认证的 Handler。</p>
<p>接下来利用 build_opener() 方法来利用这个 Handler 构建一个 Opener，那么这个 Opener 在发送请求的时候就相当于已经认证成功了。</p>
<p>接下来利用 Opener 的 open() 方法打开链接，就可以完成认证了，在这里获取到的结果就是认证后的页面源码内容。</p>
<h4 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h4><p>在做爬虫的时候免不了要使用代理， 如果要添加代理， 可以这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"></span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:9743'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:9743'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>在此本地搭建了一个代理，运行在9743端口上。</p>
<p>在这里使用了ProxyHandler， ProxyHandler的参数是一个字典，键名时协议类型， 比如HTTP还是HTTPS等， 键值时代理链接， 可以添加多个代理。</p>
<p>然后利用build_opener() 方法利用这个Handler构造一个Opener， 然后发送请求即可。</p>
<h4 id="Cookies-1"><a href="#Cookies-1" class="headerlink" title="Cookies"></a>Cookies</h4><p>Cookies的处理就需要Cookies相关的Handler了。</p>
<p>我们先用一个实例来感受以下怎样讲网站的Cookies获取下来， 代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import http.cookiejar, urllib.request</span><br><span class="line">cookie= http.cookiejar.CookieJar()</span><br><span class="line">handler= urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener= urllib.request.build_opener(handler)</span><br><span class="line">response= opener.open(&apos;http://www.baidu.com&apos;)</span><br><span class="line">For item in cookie:</span><br><span class="line">	print(item.name+&quot;=&quot;+ item.value)</span><br></pre></td></tr></table></figure>
<h2 id="Requests-库简介"><a href="#Requests-库简介" class="headerlink" title="Requests 库简介"></a>Requests 库简介</h2><p>具体的使用自己看代码。</p>
<h3 id="会话维持"><a href="#会话维持" class="headerlink" title="会话维持"></a>会话维持</h3><p>在Requests中， 我们如果直接利用get() 或 post() 等方法的确可以做到模拟网页的请求， 但是这实际上是相当于不同的会话， 即不同的session， 也就是说相当于你用了两个浏览器打开了不同的页面。</p>
<p>其实解决这个问题的主要方法就是维护同一个会话， 也就是相当于打开一个新的浏览器选项卡而不是新开一个浏览器。 但是我又不想每次设置Cookies， 那该怎么办？ 这时候就有了新的利器Session对象。</p>
<p>利用它， 我们可以方便地维护一个会话， 而且不用担心Cookies的问题， 它会帮我们自动处理好。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">requests.get(&apos;http://httpbin.org/cookies/set/number/123456789&apos;)</span><br><span class="line">r = requests.get(&apos;http://httpbin.org/cookies&apos;)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<p>在实例中我们请求了一个测试网址：<a href="http://httpbin.org/cookies/set/number/123456789" target="_blank" rel="noopener">http://httpbin.org/cookies/set/number/123456789</a>，请求这个网址我们可以设置一个 Cookie，名称叫做 number，内容是 123456789，随后又请求了<a href="http://httpbin.org/cookies" target="_blank" rel="noopener">http://httpbin.org/cookies</a>，此网址可以获取当前的 Cookies。</p>
<p>这样能成功获取到设置的 Cookies 吗？试试看。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;cookies&quot;: &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>并不行。我们再用 Session 试试看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">s = requests.Session()</span><br><span class="line">s.get(&apos;http://httpbin.org/cookies/set/number/123456789&apos;)</span><br><span class="line">r = s.get(&apos;http://httpbin.org/cookies&apos;)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<p>看下运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;cookies&quot;: &#123;</span><br><span class="line">    &quot;number&quot;: &quot;123456789&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>成功获取！这下能体会到同一个会话和不同会话的区别了吧？</p>
<p>所以，利用 Session 我们可以做到模拟同一个会话，而且不用担心 Cookies 的问题，通常用于模拟登录成功之后再进行下一步的操作。</p>
<p>Session 在平常用到的非常广泛，可以用于模拟在一个浏览器中打开同一站点的不同页面，在后文会有专门的章节来讲解这部分内容。</p>
<h3 id="SSL-证书"><a href="#SSL-证书" class="headerlink" title="SSL 证书"></a>SSL 证书</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from requests.packages import urllib3</span><br><span class="line"></span><br><span class="line">urllib3.disable_warnings()</span><br><span class="line">response = requests.get(&apos;https://www.12306.cn&apos;, verify=False)</span><br><span class="line">print(response.status_code)</span><br></pre></td></tr></table></figure>
<h3 id="设置代理"><a href="#设置代理" class="headerlink" title="设置代理"></a>设置代理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">  &apos;http&apos;: &apos;http://10.10.1.10:3128&apos;,</span><br><span class="line">  &apos;https&apos;: &apos;http://10.10.1.10:1080&apos;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">requests.get(&apos;https://www.taobao.com&apos;, proxies=proxies)</span><br></pre></td></tr></table></figure>
<p>若代理需要使用 HTTP Basic Auth，可以使用类似 <a href="http://user:password@host:port/" target="_blank" rel="noopener">http://user:password@host:port</a> 这样的语法来设置代理。</p>
<p>实例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    &apos;https&apos;: &apos;http://user:password@10.10.1.10:3128/&apos;,</span><br><span class="line">&#125;</span><br><span class="line">requests.get(&apos;https://www.taobao.com&apos;, proxies=proxies)</span><br></pre></td></tr></table></figure>
<p>除了基本的 HTTP 代理，Requests 还支持 SOCKS 协议的代理。</p>
<p>首先需要安装 Socks 这个库，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install &quot;requests[socks]&quot;</span><br></pre></td></tr></table></figure>
<p>然后就可以使用 SOCKS 协议代理了，实例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">    &apos;http&apos;: &apos;socks5://user:password@host:port&apos;,</span><br><span class="line">    &apos;https&apos;: &apos;socks5://user:password@host:port&apos;</span><br><span class="line">&#125;</span><br><span class="line">requests.get(&apos;https://www.taobao.com&apos;, proxies=proxies</span><br></pre></td></tr></table></figure>
<h3 id="身份认证"><a href="#身份认证" class="headerlink" title="身份认证"></a>身份认证</h3><p>在访问网站时，我们可能会遇到这样的认证页面，如图 3-9 所示：</p>
<p><img src="https://germey.gitbooks.io/python3webspider/content/assets/3-9.jpg" alt="img"></p>
<p>图 3-9 认证页面</p>
<p>如果遇到这样的网站验证，可以使用 Requests 自带的身份认证功能，实例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from requests.auth import HTTPBasicAuth</span><br><span class="line"></span><br><span class="line">r = requests.get(&apos;http://localhost:5000&apos;, auth=HTTPBasicAuth(&apos;username&apos;, &apos;password&apos;))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure>
<p>如果用户名和密码正确的话，请求时就会自动认证成功，会返回 200 状态码，如果认证失败，则会返回 401 状态码。</p>
<p>当然如果参数都传一个 HTTPBasicAuth 类，就显得有点繁琐了，所以 Requests 提供了一个更简单的写法，可以直接传一个元组，它会默认使用 HTTPBasicAuth 这个类来认证。</p>
<p>所以上面的代码可以直接简写如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">r = requests.get(&apos;http://localhost:5000&apos;, auth=(&apos;username&apos;, &apos;password&apos;))</span><br><span class="line">print(r.status_code)</span><br></pre></td></tr></table></figure>
<p>运行效果和上面的是一样的。</p>
<p>Requests 还提供了其他的认证方式，如 OAuth 认证，不过需要安装 oauth 包，命令如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install requests_oauthlib</span><br></pre></td></tr></table></figure>
<p>使用 OAuth1 认证的方法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from requests_oauthlib import OAuth1</span><br><span class="line"></span><br><span class="line">url = &apos;https://api.twitter.com/1.1/account/verify_credentials.json&apos;</span><br><span class="line">auth = OAuth1(&apos;YOUR_APP_KEY&apos;, &apos;YOUR_APP_SECRET&apos;,</span><br><span class="line">              &apos;USER_OAUTH_TOKEN&apos;, &apos;USER_OAUTH_TOKEN_SECRET&apos;)</span><br><span class="line">requests.get(url, auth=auth)</span><br></pre></td></tr></table></figure>
<p>更多详细的功能就可以参考 requests_oauthlib 的官方文档：<a href="https://requests-oauthlib.readthedocs.org/" target="_blank" rel="noopener">https://requests-oauthlib.readthedocs.org/</a>，在此就不再赘述了。</p>
<h3 id="Prepared-Request"><a href="#Prepared-Request" class="headerlink" title="Prepared Request"></a>Prepared Request</h3><p>在前面介绍Urllib时我们可以将Request表示为一个数据结构， Request的各个参数都可以通过一个Request对象来表示， 在Requests里面同样可以做到，这个数据结构就叫Prepared Request。</p>
<p>我们用一个实例感受一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from requests import Request, Session</span><br><span class="line"></span><br><span class="line">url = &apos;http://httpbin.org/post&apos;</span><br><span class="line">data = &#123;</span><br><span class="line">    &apos;name&apos;: &apos;germey&apos;</span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116 Safari/537.36&apos;</span><br><span class="line">&#125;</span><br><span class="line">s = Session()</span><br><span class="line">req = Request(&apos;POST&apos;, url, data=data, headers=headers)</span><br><span class="line">prepped = s.prepare_request(req)</span><br><span class="line">r = s.send(prepped)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<p>查看结果可以看到我们达到了同样的POST请求效果。</p>
<p>有了Request这个对象， 我们就可以将一个个请求当做一个独立的对象来看待， 这样在进行队列调度的时候会非常方便， 后面我们会有一节来使用它来构造一个Request队列。</p>
<h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><p>本节我们看一下正则表达式的相关用法，正则表达式是处理字符串的强大的工具，它有自己特定的语法结构，有了它，实现字符串的检索、替换、匹配验证都不在话下。</p>
<p>当然对于爬虫来说，有了它，我们从 HTML 里面提取我们想要的信息就非常方便了。</p>
<p>说了这么多，可能我们对它到底是个什么还是比较模糊，下面我们就用几个实例来感受一下正则表达式的用法。</p>
<p>我们打开开源中国提供的正则表达式测试工具：<a href="http://tool.oschina.net/regex/" target="_blank" rel="noopener">http://tool.oschina.net/regex/</a>，打开之后我们可以输入待匹配的文本，然后选择常用的正则表达式，就可以从我们输入的文本中得出相应的匹配结果了。</p>
<p>例如我们在这里输入待匹配的文本如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello, my phone number is 010-86432100 and email is cqc@cuiqingcai.com, and my website is http://cuiqingcai.com.</span><br></pre></td></tr></table></figure>
<p>这段字符串中包含了一个电话号码和一个电子邮件，接下来我们就尝试用正则表达式提取出来，如图 3-10 所示：</p>
<p><img src="https://germey.gitbooks.io/python3webspider/content/assets/3-10.jpg" alt="img"></p>
<p>图 3-10 运行页面</p>
<p>我们在网页中选择匹配 Email 地址，就可以看到在下方出现了文本中的 Email。如果我们选择了匹配网址 URL，就可以看到在下方出现了文本中的 URL。是不是非常神奇？</p>
<p>其实，在这里就是用了正则表达式匹配，也就是用了一定的规则将特定的文本提取出来。比如电子邮件它开头是一段字符串，然后是一个 @ 符号，然后就是某个域名，这是有特定的组成格式的。另外对于 URL，开头是协议类型，然后是冒号加双斜线，然后是域名加路径。</p>
<p>对于 URL 来说，我们就可以用下面的正则表达式匹配：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[a-zA-z]+://[^\s]*</span><br></pre></td></tr></table></figure>
<p>如果我们用这个正则表达式去匹配一个字符串，如果这个字符串中包含类似 URL 的文本，那就会被提取出来。</p>
<p>这个正则表达式看上去是乱糟糟的一团，其实不然，这里面都是有特定的语法规则的。比如 a-z 代表匹配任意的小写字母，\s 表示匹配任意的空白字符，* 就代表匹配前面的字符任意多个，这一长串的正则表达式就是这么多匹配规则的组合，最后实现特定的匹配功能。</p>
<p>写好正则表达式后，我们就可以拿它去一个长字符串里匹配查找了，不论这个字符串里面有什么，只要符合我们写的规则，统统可以找出来。那么对于网页来说，如果我们想找出网页源代码里有多少 URL，就可以用匹配URL的正则表达式去匹配，就可以得到源码中的 URL 了。</p>
<p>在上面我们说了几个匹配规则，那么正则表达式的规则到底有多少？那么在这里把常用的匹配规则总结一下：</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>描述</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>\w</code></td>
<td>匹配字母数字及下划线</td>
<td></td>
</tr>
<tr>
<td><code>\W</code></td>
<td>匹配非字母数字及下划线</td>
<td></td>
</tr>
<tr>
<td><code>\s</code></td>
<td>匹配任意空白字符，等价于 [\t\n\r\f].</td>
<td></td>
</tr>
<tr>
<td><code>\S</code></td>
<td>匹配任意非空字符</td>
<td></td>
</tr>
<tr>
<td><code>\d</code></td>
<td>匹配任意数字，等价于 [0-9]</td>
<td></td>
</tr>
<tr>
<td><code>\D</code></td>
<td>匹配任意非数字</td>
<td></td>
</tr>
<tr>
<td><code>\A</code></td>
<td>匹配字符串开始</td>
<td></td>
</tr>
<tr>
<td><code>\Z</code></td>
<td>匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串</td>
<td></td>
</tr>
<tr>
<td><code>\z</code></td>
<td>匹配字符串结束</td>
<td></td>
</tr>
<tr>
<td><code>\G</code></td>
<td>匹配最后匹配完成的位置</td>
<td></td>
</tr>
<tr>
<td><code>\n</code></td>
<td>匹配一个换行符</td>
<td></td>
</tr>
<tr>
<td><code>\t</code></td>
<td>匹配一个制表符</td>
<td></td>
</tr>
<tr>
<td><code>^</code></td>
<td>匹配字符串的开头</td>
<td></td>
</tr>
<tr>
<td><code>$</code></td>
<td>匹配字符串的末尾</td>
<td></td>
</tr>
<tr>
<td><code>.</code></td>
<td>匹配任意字符，除了换行符，当 re.DOTALL 标记被指定时，则可以匹配包括换行符的任意字符</td>
<td></td>
</tr>
<tr>
<td><code>[...]</code></td>
<td>用来表示一组字符，单独列出：[amk] 匹配 ‘a’，’m’ 或 ‘k’</td>
<td></td>
</tr>
<tr>
<td><code>[^...]</code></td>
<td>不在 [] 中的字符：<a href="https://germey.gitbooks.io/python3webspider/content/3.3-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.html#fn_abc" target="_blank" rel="noopener">abc</a> 匹配除了 a,b,c 之外的字符。</td>
<td></td>
</tr>
<tr>
<td><code>*</code></td>
<td>匹配 0 个或多个的表达式。</td>
<td></td>
</tr>
<tr>
<td><code>+</code></td>
<td>匹配 1 个或多个的表达式。</td>
<td></td>
</tr>
<tr>
<td><code>?</code></td>
<td>匹配 0 个或 1 个由前面的正则表达式定义的片段，非贪婪方式</td>
<td></td>
</tr>
<tr>
<td><code>{n}</code></td>
<td>精确匹配 n 个前面表达式。</td>
<td></td>
</tr>
<tr>
<td><code>{n, m}</code></td>
<td>匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式</td>
<td></td>
</tr>
<tr>
<td>`a</td>
<td>b`</td>
<td>匹配 a 或 b</td>
</tr>
<tr>
<td><code>( )</code></td>
<td>匹配括号内的表达式，也表示一个组</td>
</tr>
</tbody>
</table>
<p>可能看完了之后就有点晕晕的了把，不用担心，下面我们会详细讲解下一些常见的规则的用法。怎么用它来从网页中提取我们想要的信息。</p>
<h3 id="了解-re-库"><a href="#了解-re-库" class="headerlink" title="了解 re 库"></a>了解 re 库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;Hello 123 4567 World_This is a Regex Demo&apos;</span><br><span class="line">print(len(content))</span><br><span class="line">result = re.match(&apos;^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;&apos;, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.span())</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">41</span><br><span class="line">&lt;_sre.SRE_Match object; span=(0, 25), match=&apos;Hello 123 4567 World_This&apos;&gt;</span><br><span class="line">Hello 123 4567 World_This</span><br><span class="line">(0, 25)</span><br></pre></td></tr></table></figure>
<p>在这里我们首先声明了一个字符串，包含英文字母、空白字符、数字等等内容，接下来我们写了一个正则表达式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">^Hello\s\d\d\d\s\d&#123;4&#125;\s\w&#123;10&#125;</span><br></pre></td></tr></table></figure>
<p>用它来匹配这个长字符串。开头的 ^ 是匹配字符串的开头，也就是以 Hello 开头，然后 \s 匹配空白字符，用来匹配目标字符串的空格，\d 匹配数字，3 个 \d 匹配 123，然后再写 1 个 \s 匹配空格，后面还有 4567，我们其实可以依然用 4 个 \d 来匹配，但是这么写起来比较繁琐，所以在后面可以跟 {4} 代表匹配前面的规则 4 次，也就是匹配 4 个数字，这样也可以完成匹配，然后后面再紧接 1 个空白字符，然后 \w{10} 匹配 10 个字母及下划线，正则表达式到此为止就结束了，我们注意到其实并没有把目标字符串匹配完，不过这样依然可以进行匹配，只不过匹配结果短一点而已。</p>
<p>我们调用 match() 方法，第一个参数传入了正则表达式，第二个参数传入了要匹配的字符串。</p>
<p>打印输出一下结果，可以看到结果是 SRE_Match 对象，证明成功匹配，它有两个方法，group() 方法可以输出匹配到的内容，结果是 Hello 123 4567 World_This，这恰好是我们正则表达式规则所匹配的内容，span() 方法可以输出匹配的范围，结果是 (0, 25)，这个就是匹配到的结果字符串在原字符串中的位置范围。</p>
<p>通过上面的例子我们可以基本了解怎样在 Python 中怎样使用正则表达式来匹配一段文字。</p>
<p>刚才我们用了 match() 方法可以得到匹配到的字符串内容，但是如果我们想从字符串中提取一部分内容怎么办呢？就像最前面的实例一样，从一段文本中提取出邮件或电话号等内容。</p>
<p>在这里可以使用 () 括号来将我们想提取的子字符串括起来，() 实际上就是标记了一个子表达式的开始和结束位置，被标记的每个子表达式会依次对应每一个分组，我们可以调用 group() 方法传入分组的索引即可获取提取的结果。</p>
<p>下面我们用一个实例感受一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;Hello 1234567 World_This is a Regex Demo&apos;</span><br><span class="line">result = re.match(&apos;^Hello\s(\d+)\sWorld&apos;, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.group(1))</span><br><span class="line">print(result.span())</span><br></pre></td></tr></table></figure>
<p>依然是前面的字符串，在这里我们想匹配这个字符串并且把其中的 1234567 提取出来，在这里我们将数字部分的正则表达式用 () 括起来，然后接下来调用了group(1) 获取匹配结果。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;_sre.SRE_Match object; span=(0, 19), match=&apos;Hello 1234567 World&apos;&gt;</span><br><span class="line">Hello 1234567 World</span><br><span class="line">1234567</span><br><span class="line">(0, 19)</span><br></pre></td></tr></table></figure>
<p>可以看到在结果中成功得到了 1234567，我们获取用的是group(1)，与 group() 有所不同，group() 会输出完整的匹配结果，而 group(1) 会输出第一个被 () 包围的匹配结果，假如正则表达式后面还有 () 包括的内容，那么我们可以依次用 group(2)、group(3) 等来依次获取。</p>
<h4 id="通用匹配"><a href="#通用匹配" class="headerlink" title="通用匹配"></a>通用匹配</h4><p>刚才我们写的正则表达式其实比较复杂，出现空白字符我们就写 \s 匹配空白字符，出现数字我们就写 \d 匹配数字，工作量非常大，其实完全没必要这么做，还有一个万能匹配可以用，也就是 .<em> （点星），.（点）可以匹配任意字符（除换行符），</em>（星） 又代表匹配前面的字符无限次，所以它们组合在一起就可以匹配任意的字符了，有了它我们就不用挨个字符地匹配了。</p>
<p>所以接着上面的例子，我们可以改写一下正则表达式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;Hello 123 4567 World_This is a Regex Demo&apos;</span><br><span class="line">result = re.match(&apos;^Hello.*Demo$&apos;, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group())</span><br><span class="line">print(result.span())</span><br></pre></td></tr></table></figure>
<p>在这里我们将中间的部分直接省略，全部用 .* 来代替，最后加一个结尾字符串就好了，运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;_sre.SRE_Match object; span=(0, 41), match=&apos;Hello 123 4567 World_This is a Regex Demo&apos;&gt;</span><br><span class="line">Hello 123 4567 World_This is a Regex Demo</span><br><span class="line">(0, 41)</span><br></pre></td></tr></table></figure>
<p>可以看到 group() 方法输出了匹配的全部字符串，也就是说我们写的正则表达式匹配到了目标字符串的全部内容，span() 方法输出 (0, 41)，是整个字符串的长度。</p>
<p>因此，我们可以在使用 .* 来简化正则表达式的书写。</p>
<h4 id="贪婪与非贪婪"><a href="#贪婪与非贪婪" class="headerlink" title="贪婪与非贪婪"></a>贪婪与非贪婪</h4><p>在使用上面的通用匹配 .* 的时候可能我们有时候匹配到的并不是想要的结果，我们看下面的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;Hello 1234567 World_This is a Regex Demo&apos;</span><br><span class="line">result = re.match(&apos;^He.*(\d+).*Demo$&apos;, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group(1))</span><br></pre></td></tr></table></figure>
<p>在这里我们依然是想获取中间的数字，所以中间我们依然写的是 (\d+)，数字两侧由于内容比较杂乱，所以两侧我们想省略来写，都写 .<em>，最后组成 ^He.</em>(\d+).*Demo$，看样子并没有什么问题，我们看下运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt;</span><br><span class="line">7</span><br></pre></td></tr></table></figure>
<p>奇怪的事情发生了，我们只得到了 7 这个数字，这是怎么回事？</p>
<p>这里就涉及一个贪婪匹配与非贪婪匹配的原因了，贪婪匹配下，.<em> 会匹配尽可能多的字符，我们的正则表达式中 .</em> 后面是 \d+，也就是至少一个数字，并没有指定具体多少个数字，所以 .* 就尽可能匹配多的字符，所以它把 123456 也匹配了，给 \d+ 留下一个可满足条件的数字 7，所以 \d+ 得到的内容就只有数字 7 了。</p>
<p>但这样很明显会给我们的匹配带来很大的不便，有时候匹配结果会莫名其妙少了一部分内容。其实这里我们只需要使用非贪婪匹配匹配就好了，非贪婪匹配的写法是 .*?，多了一个 ?，那么它可以达到怎样的效果？我们再用一个实例感受一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;Hello 1234567 World_This is a Regex Demo&apos;</span><br><span class="line">result = re.match(&apos;^He.*?(\d+).*Demo$&apos;, content)</span><br><span class="line">print(result)</span><br><span class="line">print(result.group(1))</span><br></pre></td></tr></table></figure>
<p>在这里我们只是将第一个 .<em> 改成了 .</em>?，转变为非贪婪匹配。结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;_sre.SRE_Match object; span=(0, 40), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt;</span><br><span class="line">1234567</span><br></pre></td></tr></table></figure>
<p>这下我们就可以成功获取 1234567 了。原因可想而知，贪婪匹配是尽可能匹配多的字符，非贪婪匹配就是尽可能匹配少的字符，.<em>? 之后是 \d+ 用来匹配数字，当 .</em>? 匹配到 Hello 后面的空白字符的时候，再往后的字符就是数字了，而 \d+ 恰好可以匹配，那么这里 .<em>? 就不再进行匹配，交给 \d+ 去匹配后面的数字。所以这样，.</em>? 匹配了尽可能少的字符，\d+ 的结果就是 1234567 了。</p>
<p>所以说，在做匹配的时候，字符串中间我们可以尽量使用非贪婪匹配来匹配，也就是用 .<em>? 来代替 .</em>，以免出现匹配结果缺失的情况。</p>
<p>但这里注意，如果匹配的结果在字符串结尾，.*? 就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;http://weibo.com/comment/kEraCN&apos;</span><br><span class="line">result1 = re.match(&apos;http.*?comment/(.*?)&apos;, content)</span><br><span class="line">result2 = re.match(&apos;http.*?comment/(.*)&apos;, content)</span><br><span class="line">print(&apos;result1&apos;, result1.group(1))</span><br><span class="line">print(&apos;result2&apos;, result2.group(1))</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result1 </span><br><span class="line">result2 kEraCN</span><br></pre></td></tr></table></figure>
<p>观察到 .<em>? 没有匹配到任何结果，而 .</em> 则尽量匹配多的内容，成功得到了匹配结果。</p>
<p>所以在这里好好体会一下贪婪匹配和非贪婪匹配的原理，对后面写正则表达式非常有帮助。</p>
<h4 id="修饰符"><a href="#修饰符" class="headerlink" title="修饰符"></a>修饰符</h4><p>正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。</p>
<p>我们用一个实例先来感受一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;&apos;&apos;Hello 1234567 World_This</span><br><span class="line">is a Regex Demo</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">result = re.match(&apos;^He.*?(\d+).*?Demo$&apos;, content)</span><br><span class="line">print(result.group(1))</span><br></pre></td></tr></table></figure>
<p>和上面的例子相仿，我们在字符串中加了个换行符，正则表达式也是一样的来匹配其中的数字，看一下运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">AttributeError Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-18-c7d232b39645&gt; in &lt;module&gt;()</span><br><span class="line">      5 &apos;&apos;&apos;</span><br><span class="line">      6 result = re.match(&apos;^He.*?(\d+).*?Demo$&apos;, content)</span><br><span class="line">----&gt; 7 print(result.group(1))</span><br><span class="line"></span><br><span class="line">AttributeError: &apos;NoneType&apos; object has no attribute &apos;group&apos;</span><br></pre></td></tr></table></figure>
<p>运行直接报错，也就是说正则表达式没有匹配到这个字符串，返回结果为 None，而我们又调用了 group() 方法所以导致AttributeError。</p>
<p>那我们加了一个换行符为什么就匹配不到了呢？是因为 . 匹配的是除换行符之外的任意字符，当遇到换行符时，.*? 就不能匹配了，所以导致匹配失败。</p>
<p>那么在这里我们只需要加一个修饰符 re.S，即可修正这个错误。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = re.match(&apos;^He.*?(\d+).*?Demo$&apos;, content, re.S)</span><br></pre></td></tr></table></figure>
<p>在 match() 方法的第三个参数传入 re.S，它的作用是使 . 匹配包括换行符在内的所有字符。</p>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1234567</span><br></pre></td></tr></table></figure>
<p>这个 re.S 在网页匹配中会经常用到，因为 HTML 节点经常会有换行，加上它我们就可以匹配节点与节点之间的换行了。</p>
<p>另外还有一些修饰符，在必要的情况下也可以使用：</p>
<table>
<thead>
<tr>
<th>修饰符</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>re.I</td>
<td>使匹配对大小写不敏感</td>
</tr>
<tr>
<td>re.L</td>
<td>做本地化识别（locale-aware）匹配</td>
</tr>
<tr>
<td>re.M</td>
<td>多行匹配，影响 ^ 和 $</td>
</tr>
<tr>
<td>re.S</td>
<td>使 . 匹配包括换行在内的所有字符</td>
</tr>
<tr>
<td>re.U</td>
<td>根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B.</td>
</tr>
<tr>
<td>re.X</td>
<td>该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。</td>
</tr>
</tbody>
</table>
<p>在网页匹配中较为常用的为 re.S、re.I。</p>
<h4 id="转义匹配"><a href="#转义匹配" class="headerlink" title="转义匹配"></a>转义匹配</h4><p>我们知道正则表达式定义了许多匹配模式，如 . 匹配除换行符以外的任意字符，但是如果目标字符串里面它就包含 . 我们改怎么匹配？</p>
<p>那么这里就需要用到转义匹配了，我们用一个实例来感受一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;(百度)www.baidu.com&apos;</span><br><span class="line">result = re.match(&apos;\(百度\)www\.baidu\.com&apos;, content)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>当遇到用于正则匹配模式的特殊字符时，我们在前面加反斜线来转义一下就可以匹配了。例如 . 我们就可以用 . 来匹配，运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;_sre.SRE_Match object; span=(0, 17), match=&apos;(百度)www.baidu.com&apos;&gt;</span><br></pre></td></tr></table></figure>
<p>可以看到成功匹配到了原字符串。</p>
<p>以上是写正则表达式常用的几个知识点，熟练掌握上面的知识点对后面我们写正则表达式匹配非常有帮助。</p>
<h3 id="search"><a href="#search" class="headerlink" title="search()"></a>search()</h3><p>我们在前面提到过 match() 方法是从字符串的开头开始匹配，一旦开头不匹配，那么整个匹配就失败了。</p>
<p>我们看下面的例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;Extra stings Hello 1234567 World_This is a Regex Demo Extra stings&apos;</span><br><span class="line">result = re.match(&apos;Hello.*?(\d+).*?Demo&apos;, content)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>在这里我们有一个字符串，它是以 Extra 开头的，但是正则表达式我们是以 Hello 开头的，整个正则表达式是字符串的一部分，但是这样匹配是失败的，也就是说只要第一个字符不匹配整个匹配就不能成功，运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">None</span><br></pre></td></tr></table></figure>
<p>所以 match() 方法在我们在使用的时候需要考虑到开头的内容，所以在做匹配的时候并不那么方便，它适合来检测某个字符串是否符合某个正则表达式的规则。</p>
<p>所以在这里就有另外一个方法 search()，它在匹配时会扫描整个字符串，然后返回第一个成功匹配的结果，也就是说，正则表达式可以是字符串的一部分，在匹配时，search() 方法会依次扫描字符串，直到找到第一个符合规则的字符串，然后返回匹配内容，如果搜索完了还没有找到，那就返回 None。</p>
<p>我们把上面的代码中的 match() 方法修改成 search()，再看下运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;_sre.SRE_Match object; span=(13, 53), match=&apos;Hello 1234567 World_This is a Regex Demo&apos;&gt;</span><br><span class="line">1234567</span><br></pre></td></tr></table></figure>
<p>这样就得到了匹配结果。</p>
<p>所以说，为了匹配方便，我们可以尽量使用 search() 方法。</p>
<p>下面我们再用几个实例来感受一下 search() 方法的用法。</p>
<p>首先这里有一段待匹配的 HTML 文本，我们接下来写几个正则表达式实例来实现相应信息的提取。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">html = &apos;&apos;&apos;&lt;div id=&quot;songs-list&quot;&gt;</span><br><span class="line">    &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt;</span><br><span class="line">    &lt;p class=&quot;introduction&quot;&gt;</span><br><span class="line">        经典老歌列表</span><br><span class="line">    &lt;/p&gt;</span><br><span class="line">    &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt;</span><br><span class="line">        &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;7&quot;&gt;</span><br><span class="line">            &lt;a href=&quot;/2.mp3&quot; singer=&quot;任贤齐&quot;&gt;沧海一声笑&lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt;</span><br><span class="line">            &lt;a href=&quot;/3.mp3&quot; singer=&quot;齐秦&quot;&gt;往事随风&lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;6&quot;&gt;&lt;a href=&quot;/4.mp3&quot; singer=&quot;beyond&quot;&gt;光辉岁月&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;5&quot;&gt;&lt;a href=&quot;/5.mp3&quot; singer=&quot;陈慧琳&quot;&gt;记事本&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;5&quot;&gt;</span><br><span class="line">            &lt;a href=&quot;/6.mp3&quot; singer=&quot;邓丽君&quot;&gt;&lt;i class=&quot;fa fa-user&quot;&gt;&lt;/i&gt;但愿人长久&lt;/a&gt;</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">&lt;/div&gt;&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
<p>观察到 ul 节点里面有许多 li 节点，其中 li 节点有的包含 a 节点，有的不包含 a 节点，a 节点还有一些相应的属性，超链接和歌手名。</p>
<p>首先我们尝试提取 class 为 active的 li 节点内部的超链接包含的歌手名和歌名。</p>
<p>所以我们需要提取第三个 li 节点下的 a 节点的 singer 属性和文本。</p>
<p>所以正则表达式可以以 li 开头，然后接下来寻找一个标志符 active，中间的部分可以用 .<em>? 来匹配，然后接下来我们要提取 singer 这个属性值，所以还需要写入singer=”(.</em>?)” ，我们需要提取的部分用小括号括起来，以便于用 group() 方法提取出来，它的两侧边界是双引号，然后接下来还需要匹配 a 节点的文本，那么它的左边界是 &gt;，右边界是 \，所以我们指定一下左右边界，然后目标内容依然用 (.*?) 来匹配，所以最后的正则表达式就变成了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;li.*?active.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;</span><br></pre></td></tr></table></figure>
<p>然后我们再调用 search() 方法，它便会搜索整个 HTML 文本，找到符合正则表达式的第一个内容返回。 另外由于代码有换行，所以这里第三个参数需要传入 re.S。</p>
<p>所以整个匹配代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = re.search(&apos;&lt;li.*?active.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html, re.S)</span><br><span class="line">if result:</span><br><span class="line">    print(result.group(1), result.group(2))</span><br></pre></td></tr></table></figure>
<p>由于我们需要获取的歌手和歌名都已经用了小括号包围，所以可以用 group() 方法获取，序号依次对应 group() 的参数。</p>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">齐秦 往事随风</span><br></pre></td></tr></table></figure>
<p>可以看到这个正是我们想提取的 class 为 active 的 li 节点内部的超链接包含的歌手名和歌名。</p>
<p>那么正则表达式不加 active 会怎样呢？也就是匹配不带 class 为 active 的节点内容，我们将正则表达式中的 active 去掉，代码改写如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = re.search(&apos;&lt;li.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html, re.S)</span><br><span class="line">if result:</span><br><span class="line">    print(result.group(1), result.group(2))</span><br></pre></td></tr></table></figure>
<p>由于 search() 方法会返回第一个符合条件的匹配目标，那在这里结果就变了。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">任贤齐 沧海一声笑</span><br></pre></td></tr></table></figure>
<p>因为我们把 active 标签去掉之后，从字符串开头开始搜索，符合条件的节点就变成了第二个 li 节点，后面的就不再进行匹配，所以运行结果自然就变成了第二个 li 节点中的内容。</p>
<p>注意在上面两次匹配中，search() 方法的第三个参数我们都加了 re.S，使得 .*? 可以匹配换行，所以含有换行的 li 节点被匹配到了，如果我们将其去掉，结果会是什么？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = re.search(&apos;&lt;li.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html)</span><br><span class="line">if result:</span><br><span class="line">    print(result.group(1), result.group(2))</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beyond 光辉岁月</span><br></pre></td></tr></table></figure>
<p>可以看到结果就变成了第四个 li 节点的内容，这是因为第二个和第三个 li 节点都包含了换行符，去掉 re.S 之后，.*? 已经不能匹配换行符，所以正则表达式不会匹配到第二个和第三个 li 节点，而第四个 li 节点中不包含换行符，所以成功匹配。</p>
<p>由于绝大部分的 HTML 文本都包含了换行符，所以通过上面的例子，我们尽量都需要加上 re.S 修饰符，以免出现匹配不到的问题。</p>
<h3 id="findall"><a href="#findall" class="headerlink" title="findall()"></a>findall()</h3><p>在前面我们说了 search() 方法的用法，它可以返回匹配正则表达式的第一个内容，但是如果我们想要获取匹配正则表达式的所有内容的话怎么办？这时就需要借助于 findall() 方法了。</p>
<p>findall() 方法会搜索整个字符串然后返回匹配正则表达式的所有内容。</p>
<p>还是上面的 HTML 文本，如果我们想获取所有 a 节点的超链接、歌手和歌名，就可以将 search() 方法换成 findall() 方法。如果有返回结果的话就是列表类型，所以我们需要遍历一下来获依次获取每组内容。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">results = re.findall(&apos;&lt;li.*?href=&quot;(.*?)&quot;.*?singer=&quot;(.*?)&quot;&gt;(.*?)&lt;/a&gt;&apos;, html, re.S)</span><br><span class="line">print(results)</span><br><span class="line">print(type(results))</span><br><span class="line">for result in results:</span><br><span class="line">    print(result)</span><br><span class="line">    print(result[0], result[1], result[2])</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;), (&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;), (&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;), (&apos;/5.mp3&apos;, &apos;陈慧琳&apos;, &apos;记事本&apos;), (&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;)]</span><br><span class="line">&lt;class &apos;list&apos;&gt;</span><br><span class="line">(&apos;/2.mp3&apos;, &apos;任贤齐&apos;, &apos;沧海一声笑&apos;)</span><br><span class="line">/2.mp3 任贤齐 沧海一声笑</span><br><span class="line">(&apos;/3.mp3&apos;, &apos;齐秦&apos;, &apos;往事随风&apos;)</span><br><span class="line">/3.mp3 齐秦 往事随风</span><br><span class="line">(&apos;/4.mp3&apos;, &apos;beyond&apos;, &apos;光辉岁月&apos;)</span><br><span class="line">/4.mp3 beyond 光辉岁月</span><br><span class="line">(&apos;/5.mp3&apos;, &apos;陈慧琳&apos;, &apos;记事本&apos;)</span><br><span class="line">/5.mp3 陈慧琳 记事本</span><br><span class="line">(&apos;/6.mp3&apos;, &apos;邓丽君&apos;, &apos;但愿人长久&apos;)</span><br><span class="line">/6.mp3 邓丽君 但愿人长久</span><br></pre></td></tr></table></figure>
<p>可以看到，返回的列表的每个元素都是元组类型，我们用对应的索引依次取出即可。</p>
<p>所以，如果只是获取第一个内容，可以用 search() 方法，当需要提取多个内容时，就可以用 findall() 方法。</p>
<h3 id="sub"><a href="#sub" class="headerlink" title="sub()"></a>sub()</h3><p>正则表达式除了提取信息，我们有时候还需要借助于它来修改文本，比如我们想要把一串文本中的所有数字都去掉，如果我们只用字符串的 replace() 方法那就太繁琐了，在这里我们就可以借助于 sub() 方法。</p>
<p>我们用一个实例来感受一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content = &apos;54aK54yr5oiR54ix5L2g&apos;</span><br><span class="line">content = re.sub(&apos;\d+&apos;, &apos;&apos;, content)</span><br><span class="line">print(content)</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aKyroiRixLg</span><br></pre></td></tr></table></figure>
<p>在这里我们只需要在第一个参数传入 \d+ 来匹配所有的数字，然后第二个参数是替换成的字符串，要去掉的话就可以赋值为空，第三个参数就是原字符串。</p>
<p>得到的结果就是替换修改之后的内容。</p>
<p>那么在上面的 HTML 文本中，如果我们想正则获取所有 li 节点的歌名，如果直接用正则表达式来提取可能比较繁琐，比如可以写成这样子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">results = re.findall(&apos;&lt;li.*?&gt;\s*?(&lt;a.*?&gt;)?(\w+)(&lt;/a&gt;)?\s*?&lt;/li&gt;&apos;, html, re.S)</span><br><span class="line">for result in results:</span><br><span class="line">    print(result[1])</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">一路上有你</span><br><span class="line">沧海一声笑</span><br><span class="line">往事随风</span><br><span class="line">光辉岁月</span><br><span class="line">记事本</span><br><span class="line">但愿人长久</span><br></pre></td></tr></table></figure>
<p>但如果我们借助于 sub() 方法就比较简单了，我们可以先用sub() 方法将 a 节点去掉，只留下文本，然后再利用findall() 提取就好了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">html = re.sub(&apos;&lt;a.*?&gt;|&lt;/a&gt;&apos;, &apos;&apos;, html)</span><br><span class="line">print(html)</span><br><span class="line">results = re.findall(&apos;&lt;li.*?&gt;(.*?)&lt;/li&gt;&apos;, html, re.S)</span><br><span class="line">for result in results:</span><br><span class="line">    print(result.strip())</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&lt;div id=&quot;songs-list&quot;&gt;</span><br><span class="line">    &lt;h2 class=&quot;title&quot;&gt;经典老歌&lt;/h2&gt;</span><br><span class="line">    &lt;p class=&quot;introduction&quot;&gt;</span><br><span class="line">        经典老歌列表</span><br><span class="line">    &lt;/p&gt;</span><br><span class="line">    &lt;ul id=&quot;list&quot; class=&quot;list-group&quot;&gt;</span><br><span class="line">        &lt;li data-view=&quot;2&quot;&gt;一路上有你&lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;7&quot;&gt;</span><br><span class="line">            沧海一声笑</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;4&quot; class=&quot;active&quot;&gt;</span><br><span class="line">            往事随风</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;6&quot;&gt;光辉岁月&lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;5&quot;&gt;记事本&lt;/li&gt;</span><br><span class="line">        &lt;li data-view=&quot;5&quot;&gt;</span><br><span class="line">            但愿人长久</span><br><span class="line">        &lt;/li&gt;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">一路上有你</span><br><span class="line">沧海一声笑</span><br><span class="line">往事随风</span><br><span class="line">光辉岁月</span><br><span class="line">记事本</span><br><span class="line">但愿人长久</span><br></pre></td></tr></table></figure>
<p>可以到 a 节点在经过 sub() 方法处理后都没有了，然后再 findall() 直接提取即可。所以在适当的时候我们可以借助于 sub() 方法做一些相应处理可以事半功倍。</p>
<h3 id="compile"><a href="#compile" class="headerlink" title="compile()"></a>compile()</h3><p>前面我们所讲的方法都是用来处理字符串的方法，最后再介绍一个 compile() 方法，这个方法可以讲正则字符串编译成正则表达式对象，以便于在后面的匹配中复用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">content1 = &apos;2016-12-15 12:00&apos;</span><br><span class="line">content2 = &apos;2016-12-17 12:55&apos;</span><br><span class="line">content3 = &apos;2016-12-22 13:21&apos;</span><br><span class="line">pattern = re.compile(&apos;\d&#123;2&#125;:\d&#123;2&#125;&apos;)</span><br><span class="line">result1 = re.sub(pattern, &apos;&apos;, content1)</span><br><span class="line">result2 = re.sub(pattern, &apos;&apos;, content2)</span><br><span class="line">result3 = re.sub(pattern, &apos;&apos;, content3)</span><br><span class="line">print(result1, result2, result3)</span><br></pre></td></tr></table></figure>
<p>例如这里有三个日期，我们想分别将三个日期中的时间去掉，所以在这里我们可以借助于 sub() 方法，sub() 方法的第一个参数是正则表达式，但是这里我们没有必要重复写三个同样的正则表达式，所以可以借助于 compile() 方法将正则表达式编译成一个正则表达式对象，以便复用。</p>
<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-12-15  2016-12-17  2016-12-22</span><br></pre></td></tr></table></figure>
<p>另外 compile() 还可以传入修饰符，例如 re.S 等修饰符，这样在 search()、findall() 等方法中就不需要额外传了。所以 compile() 方法可以说是给正则表达式做了一层封装，以便于我们更好地复用。</p>
<h2 id="Stack-Overflow"><a href="#Stack-Overflow" class="headerlink" title="Stack Overflow"></a>Stack Overflow</h2><h3 id="What-is-the-difference-between-PathParam-and-QueryParam"><a href="#What-is-the-difference-between-PathParam-and-QueryParam" class="headerlink" title="What is the difference between @PathParam and @QueryParam"></a>What is the difference between @PathParam and @QueryParam</h3><p>Query parameters are added to the url after the ? mark, while a path parameter is part of the regular URL.</p>
<p>n the URL below <code>tom</code> could be the value of a path parameter and there is one query parameter with the name <code>id</code> and value <code>1</code>:</p>
<p><code>http://mydomain.com/tom?id=1</code></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/09/Producer-Consumer-in-Java/" rel="next" title="Producer-Consumer in Java">
                <i class="fa fa-chevron-left"></i> Producer-Consumer in Java
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.jpg"
                alt="Liyan Chen" />
            
              <p class="site-author-name" itemprop="name">Liyan Chen</p>
              <p class="site-description motion-element" itemprop="description">千江有水千江月， 万里无云万里天</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">39</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/Lic128" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:lic128@eng.ucsd.edu" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#HTTP基本原理"><span class="nav-number">1.</span> <span class="nav-text">HTTP基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Request"><span class="nav-number">1.1.</span> <span class="nav-text">Request</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Request-Method"><span class="nav-number">1.2.</span> <span class="nav-text">Request Method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Request-URL"><span class="nav-number">1.3.</span> <span class="nav-text">Request URL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Request-Headers"><span class="nav-number">1.4.</span> <span class="nav-text">Request Headers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Request-Body"><span class="nav-number">1.5.</span> <span class="nav-text">Request Body</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Response"><span class="nav-number">1.6.</span> <span class="nav-text">Response</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Respond-Status-Code"><span class="nav-number">1.7.</span> <span class="nav-text">Respond Status Code</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Respond-Headers"><span class="nav-number">1.8.</span> <span class="nav-text">Respond Headers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Response-Body"><span class="nav-number">1.9.</span> <span class="nav-text">Response Body</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#爬虫基本原理"><span class="nav-number">2.</span> <span class="nav-text">爬虫基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#获取网页"><span class="nav-number">2.1.</span> <span class="nav-text">获取网页</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#提取信息"><span class="nav-number">2.2.</span> <span class="nav-text">提取信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#保存信息"><span class="nav-number">2.3.</span> <span class="nav-text">保存信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#能抓怎么的数据"><span class="nav-number">2.4.</span> <span class="nav-text">能抓怎么的数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#JavaScript渲染页面"><span class="nav-number">2.5.</span> <span class="nav-text">JavaScript渲染页面</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#登陆功能的实现"><span class="nav-number">2.6.</span> <span class="nav-text">登陆功能的实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Session"><span class="nav-number">2.6.1.</span> <span class="nav-text">Session</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cookies"><span class="nav-number">2.6.2.</span> <span class="nav-text">Cookies</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookies-基本结构"><span class="nav-number">2.7.</span> <span class="nav-text">Cookies 基本结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见误区"><span class="nav-number">2.8.</span> <span class="nav-text">常见误区</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#代理基本原理"><span class="nav-number">3.</span> <span class="nav-text">代理基本原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本原理"><span class="nav-number">3.1.</span> <span class="nav-text">基本原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫代理"><span class="nav-number">3.2.</span> <span class="nav-text">爬虫代理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Urllib简介"><span class="nav-number">4.</span> <span class="nav-text">Urllib简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Request-1"><span class="nav-number">4.1.</span> <span class="nav-text">Request</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Data参数"><span class="nav-number">4.1.1.</span> <span class="nav-text">Data参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Time-out-参数"><span class="nav-number">4.1.2.</span> <span class="nav-text">Time out 参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Context-参数"><span class="nav-number">4.1.3.</span> <span class="nav-text">Context 参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Request类的使用"><span class="nav-number">4.2.</span> <span class="nav-text">Request类的使用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#高级用法"><span class="nav-number">4.3.</span> <span class="nav-text">高级用法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#认证"><span class="nav-number">4.3.1.</span> <span class="nav-text">认证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代理"><span class="nav-number">4.3.2.</span> <span class="nav-text">代理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cookies-1"><span class="nav-number">4.3.3.</span> <span class="nav-text">Cookies</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Requests-库简介"><span class="nav-number">5.</span> <span class="nav-text">Requests 库简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#会话维持"><span class="nav-number">5.1.</span> <span class="nav-text">会话维持</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SSL-证书"><span class="nav-number">5.2.</span> <span class="nav-text">SSL 证书</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#设置代理"><span class="nav-number">5.3.</span> <span class="nav-text">设置代理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#身份认证"><span class="nav-number">5.4.</span> <span class="nav-text">身份认证</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Prepared-Request"><span class="nav-number">5.5.</span> <span class="nav-text">Prepared Request</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#正则表达式"><span class="nav-number">6.</span> <span class="nav-text">正则表达式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#了解-re-库"><span class="nav-number">6.1.</span> <span class="nav-text">了解 re 库</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#通用匹配"><span class="nav-number">6.1.1.</span> <span class="nav-text">通用匹配</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#贪婪与非贪婪"><span class="nav-number">6.1.2.</span> <span class="nav-text">贪婪与非贪婪</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#修饰符"><span class="nav-number">6.1.3.</span> <span class="nav-text">修饰符</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#转义匹配"><span class="nav-number">6.1.4.</span> <span class="nav-text">转义匹配</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#search"><span class="nav-number">6.2.</span> <span class="nav-text">search()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#findall"><span class="nav-number">6.3.</span> <span class="nav-text">findall()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sub"><span class="nav-number">6.4.</span> <span class="nav-text">sub()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#compile"><span class="nav-number">6.5.</span> <span class="nav-text">compile()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Stack-Overflow"><span class="nav-number">7.</span> <span class="nav-text">Stack Overflow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-is-the-difference-between-PathParam-and-QueryParam"><span class="nav-number">7.1.</span> <span class="nav-text">What is the difference between @PathParam and @QueryParam</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liyan Chen</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  


  




	





  





  












  





  

  

  

  
  

  
  


  

  

</body>
</html>
